{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-rR3bJasIvX"
   },
   "source": [
    "# Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wirfzkkEsJW4"
   },
   "outputs": [],
   "source": [
    "#import packages here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "N_STATES = 6   # the width of 1-dim world\n",
    "ACTIONS = ['left', 'right']     # the available actions to use\n",
    "EPSILON = 0.9   # the degree of greedy (0＜ε＜1)\n",
    "ALPHA = 0.1     # learning rate (0＜α≤1)\n",
    "GAMMA = 0.9    # discount factor (0＜γ＜1)\n",
    "MAX_EPOCHES = 13   # the max epoches\n",
    "FRESH_TIME = 0.3    # the interval time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TbdlOJB7sOn7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n"
     ]
    }
   ],
   "source": [
    "#define the function here\n",
    "def build_q_table(n_states, actions):\n",
    "    df = pd.DataFrame(data=0.0, index=range(n_states), columns=actions)\n",
    "    return df\n",
    "\n",
    "q_table = build_q_table(N_STATES, ACTIONS)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "OtPWgEtosVho"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n"
     ]
    }
   ],
   "source": [
    "#define the function here\n",
    "# Given state and Q-table, choose action\n",
    "def choose_action(state, q_table):\n",
    "    # pick all actions from this state\n",
    "    rand = np.random.random()\n",
    "    #print(rand)\n",
    "    if (q_table.loc[state] == 0).all() or rand>EPSILON:  # non-greedy or non-explored\n",
    "        action_name = np.random.choice(q_table.columns)\n",
    "    else:\n",
    "        action_name = q_table.loc[state].idxmax()\n",
    "    return action_name\n",
    "\n",
    "sample_action = choose_action(0, q_table)\n",
    "print(sample_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "qr1W3h6ksY7V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n"
     ]
    }
   ],
   "source": [
    "#define the function here\n",
    "def get_env_feedback(S_current, A):\n",
    "    # This is how agent will interact with the environment\n",
    "    if A == 'right':    # move right\n",
    "        if S_current == 4:\n",
    "            S_next = 'terminal'\n",
    "            R = 1\n",
    "        else:\n",
    "            S_next = S_current+1\n",
    "            R = 0\n",
    "    else:   # move left\n",
    "        if S_current == 0:\n",
    "            S_next = 0\n",
    "            R = 0\n",
    "        else:\n",
    "            S_next = S_current-1\n",
    "            R = 0\n",
    "    return S_next, R\n",
    "\n",
    "sample_action = 'left'\n",
    "S_current = 4\n",
    "sample_feedback = get_env_feedback(S_current, sample_action)\n",
    "print(sample_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "pBkBKYpDsbxK"
   },
   "outputs": [],
   "source": [
    "def update_env(S, episode, step_counter):\n",
    "#     # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = '  Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('{}\\n'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "kW_umQ6bsf86"
   },
   "outputs": [],
   "source": [
    "#define the function here\n",
    "def reinforce_learning():\n",
    "    \n",
    "#     # main part of RL loop\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "#     ...\n",
    "#     #start training loop\n",
    "    for episode in range(MAX_EPOCHES):       \n",
    "        step_counter = 0  #counter for counting steps to reach the treasure\n",
    "        S_current = 0    #start from S_current\n",
    "        is_terminated = False   #flag to conrinue or stop the loop\n",
    "        update_env(S_current, episode, step_counter)   #update environment\n",
    "        while not is_terminated:\n",
    "            A = choose_action(S_current, q_table)\n",
    "            S_next, reward = get_env_feedback(S_current, A)\n",
    "            \n",
    "            ...#update Q-table\n",
    "            # Treasure not found yet\n",
    "            if S_next != 'terminal':  \n",
    "                # Use the bellman equation to calc value\n",
    "                q_target = reward+GAMMA * q_table.loc[S_next, :].max()  \n",
    "            else:\n",
    "                # Next is treasure so set value to max\n",
    "                q_target = 1  \n",
    "                is_terminated = True  \n",
    "\n",
    "            q_table.loc[S_current, A] += ALPHA * (q_target - q_table.loc[S_current, A])\n",
    "            ...  # move to next state\n",
    "            S_current = S_next\n",
    "\n",
    "            update_env(S_current, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "\n",
    "    return q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "8U520WbcsjPS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----oT  Episode 1: total_steps = 5\n",
      "----oT  Episode 2: total_steps = 25\n",
      "----oT  Episode 3: total_steps = 34\n",
      "----oT  Episode 4: total_steps = 11\n",
      "----oT  Episode 5: total_steps = 6\n",
      "----oT  Episode 6: total_steps = 5\n",
      "----oT  Episode 7: total_steps = 5\n",
      "----oT  Episode 8: total_steps = 7\n",
      "----oT  Episode 9: total_steps = 5\n",
      "----oT  Episode 10: total_steps = 7\n",
      "----oT  Episode 11: total_steps = 5\n",
      "----oT  Episode 12: total_steps = 5\n",
      "----oT  Episode 13: total_steps = 5\n",
      "\n",
      "Q-table:\n",
      "\n",
      "       left     right\n",
      "0  0.000000  0.004438\n",
      "1  0.000000  0.027315\n",
      "2  0.000000  0.123671\n",
      "3  0.000000  0.379680\n",
      "4  0.037197  0.745813\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "#main function to run\n",
    "if __name__ == \"__main__\":\n",
    "    q_table = reinforce_learning()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
